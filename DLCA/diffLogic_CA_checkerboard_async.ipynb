{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMamlEAbKOrO"
   },
   "source": [
    "# Differentiable Logic Cellular Automata: from Game of Life to Pattern Generation\n",
    "\n",
    "This notebook contains code to reproduce experiments and figures for the\n",
    "\"Differentiable Logic Cellular Automata: from Game of Life to Pattern\n",
    "Generation\" article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Checkerboard (Asynchronous Training)\n",
    "\n",
    "This notebook trains a DiffLogic CA to generate a checkerboard pattern using asynchronous updates (random update masks). It includes training, visualization, and robustness/self-healing tests under asynchronous dynamics.\n",
    "\n",
    "This notebook is standalone and includes the shared setup code from the original DiffLogic CA notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6Tv460dKM3O"
   },
   "source": [
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use\n",
    "this file except in compliance with the License. You may obtain a copy of the\n",
    "License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or\n",
    "agreed to in writing, software distributed under the License is distributed on\n",
    "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n",
    "or implied. See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q_ToewoMBfk"
   },
   "source": [
    "**Note for the reader (as of March 20, 2025):**: We tested reproducibility of the results reported in\n",
    "this article using publicly available Colab notebooks with T4 GPUs. Determinism\n",
    "is ensured by setting the os.environ['XLA_FLAGS'] = '\n",
    "--xla_gpu_deterministic_ops=true' flag, which resolves non-determinism caused by\n",
    "specific JAX optimization routines, though at the cost of speed and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxZse8uV-C3D"
   },
   "outputs": [],
   "source": [
    "# We tested reproducibility on jax version 0.4.33\n",
    "\n",
    "!pip install jax[cuda12_pip]==0.4.33\n",
    "!pip install jaxlib==0.4.33\n",
    "\n",
    "# You might need to restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bc3071e67b59"
   },
   "outputs": [],
   "source": [
    "# @title Patch extraction backend\n",
    "# Set to False on older GPUs that fail cuDNN autotuning.\n",
    "USE_CUDNN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ii6ioJvju78Q"
   },
   "outputs": [],
   "source": [
    "\"\"\"Reproducibility:\n",
    "\n",
    "This flag is necessary for ensuring reproducibility on Colab.\n",
    "Removing it significantly boosts performance (5-10x) but compromises\n",
    "result consistency due to inherent GPU non-determinism.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "# os.environ['XLA_FLAGS'] = ' --xla_gpu_deterministic_ops=true'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uWYAAFqsDV_Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.1\n"
     ]
    }
   ],
   "source": [
    "# @title Imports and Notebook Utilities\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import io\n",
    "import itertools\n",
    "from base64 import b64decode\n",
    "from typing import List, Optional, Sequence, Tuple\n",
    "\n",
    "from einops import rearrange\n",
    "import flax.linen as nn\n",
    "import flax.linen as nn\n",
    "from IPython.display import HTML, Image, clear_output\n",
    "import jax\n",
    "print(jax.__version__)\n",
    "from jax import grad\n",
    "from jax.lax import conv_general_dilated_patches\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from jax.tree_util import tree_leaves, tree_map\n",
    "from matplotlib import colors\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "import optax\n",
    "import PIL\n",
    "import PIL.Image, PIL.ImageDraw, PIL.ImageFont, PIL.ImageOps\n",
    "import requests\n",
    "\n",
    "\n",
    "os.environ['FFMPEG_BINARY'] = 'ffmpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FYKUs0uhu78Q"
   },
   "outputs": [],
   "source": [
    "# @title Hyperparameters\n",
    "\n",
    "# Index of the pass-through gate used for network initialization.\n",
    "PASS_THROUGH_GATE = 3\n",
    "DEFAULT_PASS_VALUE = 10.0\n",
    "\n",
    "# Number of possible gates with 2 inputs and 1 output.\n",
    "NUMBER_OF_GATES = 16\n",
    "\n",
    "\"\"\"\n",
    "Fire rate for asynchronicity.\n",
    "\n",
    "60% of cells will perform the update,\n",
    "40% will have the update masked.\n",
    "\"\"\"\n",
    "FIRE_RATE = 0.6\n",
    "\n",
    "TARGET_EMOJI = \"ðŸ¦Ž\"\n",
    "TARGET_SIZE_EMOJI = 20\n",
    "TARGET_SIZE_G = 16\n",
    "TARGET_SIZE_ASYNC = 14\n",
    "TARGET_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "i1xwG6JvEeeI"
   },
   "outputs": [],
   "source": [
    "# @title Utils and Plotting\n",
    "\n",
    "\n",
    "def zoom(img, scale=4):\n",
    "  img = np.repeat(img, scale, 0)\n",
    "  img = np.repeat(img, scale, 1)\n",
    "  return img\n",
    "\n",
    "\n",
    "def load_byte(byte, max_size=TARGET_SIZE):\n",
    "  img = PIL.Image.open(io.BytesIO(byte))\n",
    "  img.thumbnail((max_size, max_size), PIL.Image.LANCZOS)\n",
    "  img = np.float32(img) / 255.0\n",
    "  # premultiply RGB by Alpha\n",
    "  img[..., :3] *= img[..., 3:]\n",
    "  return img\n",
    "\n",
    "def visualize(frames, namefile, size, negate=1):\n",
    "  # Create and save animation\n",
    "  fig, ax = plt.subplots(figsize=(size, size))\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "\n",
    "  def animate(frame):\n",
    "    ax.clear()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    if negate:\n",
    "      frame = 1 - frame\n",
    "\n",
    "    return [ax.imshow(frame, cmap='binary')]\n",
    "\n",
    "  anim = animation.FuncAnimation(fig, animate, frames=frames, interval=200, blit=True)\n",
    "\n",
    "  # Save the gif\n",
    "  writer = animation.PillowWriter(fps=2)\n",
    "  anim.save(namefile, writer=writer)\n",
    "  plt.close()\n",
    "\n",
    "\n",
    "def plot_show(img, negate=1):\n",
    "  if negate:\n",
    "    img = 1 - img\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "  ax.imshow(img, cmap='binary')\n",
    "  ax.axis('off')\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "# Plot the histogram related to the statistics of the gates.\n",
    "def plot_hist_gates(\n",
    "    net, save_path=None, title='Distribution of Gates (Sorted)'\n",
    "):\n",
    "  gate_types = [\n",
    "      'FALSE',\n",
    "      'AND',\n",
    "      'A AND (NOT B)',\n",
    "      'A',\n",
    "      '(NOT A) AND B',\n",
    "      'B',\n",
    "      'XOR',\n",
    "      'OR',\n",
    "      'NOR',\n",
    "      'XNOR',\n",
    "      'NOT B',\n",
    "      'A OR (NOT B)',\n",
    "      'NOT A',\n",
    "      '(NOT A) OR B',\n",
    "      'NAND',\n",
    "      'TRUE',\n",
    "  ]\n",
    "\n",
    "  gate_counts = np.bincount(net, minlength=len(gate_types))\n",
    "  sorted_indices = np.argsort(gate_counts)\n",
    "  sorted_counts = gate_counts[sorted_indices]\n",
    "  sorted_gate_types = [gate_types[i] for i in sorted_indices]\n",
    "  fig, ax = plt.subplots(figsize=(12, 6), dpi=100)\n",
    "\n",
    "  ax.barh(\n",
    "      sorted_gate_types,\n",
    "      sorted_counts,\n",
    "      color='#2E86C1',\n",
    "      alpha=0.7,\n",
    "      edgecolor='white',\n",
    "      linewidth=1,\n",
    "  )\n",
    "\n",
    "  ax.set_xlabel('#Gates', fontsize=12, fontweight='bold')\n",
    "  ax.set_ylabel('Type of gate', fontsize=12, fontweight='bold')\n",
    "  ax.set_title(\n",
    "      'Distribution of Gates (Sorted)', fontsize=14, fontweight='bold', pad=20\n",
    "  )\n",
    "\n",
    "  plt.style.use('seaborn-v0_8-darkgrid')\n",
    "  ax.grid(True, linestyle='--', alpha=0.7, axis='x')\n",
    "  ax.spines['top'].set_visible(False)\n",
    "  ax.spines['right'].set_visible(False)\n",
    "  ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "  ax.set_facecolor('#f8f9fa')\n",
    "  fig.set_facecolor('white')\n",
    "  plt.tight_layout()\n",
    "\n",
    "  if save_path:\n",
    "    with open(save_path, 'wb') as f:\n",
    "      plt.savefig(f, format='svg')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "# Plot the training loss functions.\n",
    "def plot_training_progress(\n",
    "    loss_train, loss_test, compute_every, save_path=None\n",
    "):\n",
    "  plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "\n",
    "  ax.grid(True, color='gray', linestyle='-', linewidth=0.8, alpha=0.3)\n",
    "\n",
    "  ax.plot(\n",
    "      loss_train,\n",
    "      color='#2E86C1',\n",
    "      linewidth=2,\n",
    "      label='Soft Gates Loss',\n",
    "      alpha=0.9,\n",
    "  )\n",
    "\n",
    "  ax.plot(\n",
    "      np.arange(0, len(loss_train), compute_every),\n",
    "      loss_test,\n",
    "      color='#E74C3C',\n",
    "      linestyle='--',\n",
    "      linewidth=2,\n",
    "      label='Hard Gates Loss',\n",
    "      alpha=0.9,\n",
    "  )\n",
    "\n",
    "  ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "  ax.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "  ax.set_ylabel('Loss Value', fontsize=12, fontweight='bold')\n",
    "  ax.set_title(\n",
    "      'Training Progress - Soft vs Hard Gates',\n",
    "      fontsize=14,\n",
    "      fontweight='bold',\n",
    "      pad=20,\n",
    "  )\n",
    "\n",
    "  ax.legend(\n",
    "      frameon=True, fancybox=True, shadow=True, fontsize=10, loc='upper right'\n",
    "  )\n",
    "\n",
    "  for spine in ax.spines.values():\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "  ax.set_facecolor('#f8f9fa')\n",
    "  fig.set_facecolor('white')\n",
    "\n",
    "  plt.tight_layout()\n",
    "\n",
    "  if save_path:\n",
    "    with open(save_path, 'wb') as f:\n",
    "      plt.savefig(f, format='svg')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bNkdywCOEir4"
   },
   "outputs": [],
   "source": [
    "# @title Model definition\n",
    "\n",
    "\n",
    "def get_moore_connections(key):\n",
    "  \"\"\"Generate Moore neighborhood connections for a 9x1 vector.\n",
    "\n",
    "  Center element is at index 4 and connects to all other elements.\n",
    "  \"\"\"\n",
    "  neighbors = jnp.array([0, 1, 2, 3, 5, 6, 7, 8])\n",
    "  a = neighbors\n",
    "  b = jnp.full_like(neighbors, 4)\n",
    "  perm = jax.random.permutation(key, neighbors.shape[0])\n",
    "  a = a[perm]\n",
    "  b = b[perm]\n",
    "  return a, b\n",
    "\n",
    "\n",
    "# From https://github.com/Felix-Petersen/difflogic/tree/main/difflogic\n",
    "def get_unique_connections(in_dim, out_dim, key):\n",
    "  assert (\n",
    "      out_dim * 2 >= in_dim\n",
    "  )  # Number of neurons must not be smaller than half of inputs\n",
    "  x = jnp.arange(in_dim)\n",
    "  # Take pairs (0, 1), (2, 3), (4, 5), ...\n",
    "  a = x[::2]\n",
    "  b = x[1::2]\n",
    "  m = min(a.shape[0], b.shape[0])\n",
    "  a = a[:m]\n",
    "  b = b[:m]\n",
    "  # If needed, add pairs (1, 2), (3, 4), (5, 6), ...\n",
    "  if a.shape[0] < out_dim:\n",
    "    a_ = x[1::2]\n",
    "    b_ = x[2::2]\n",
    "    m = min(a_.shape[0], b_.shape[0])\n",
    "    a = jnp.concatenate([a, a_[:m]])\n",
    "    b = jnp.concatenate([b, b_[:m]])\n",
    "  # If still needed, add pairs with larger offsets\n",
    "  offset = 2\n",
    "  while out_dim > a.shape[0] and offset < in_dim:\n",
    "    a_ = x[:-offset]\n",
    "    b_ = x[offset:]\n",
    "    a = jnp.concatenate([a, a_])\n",
    "    b = jnp.concatenate([b, b_])\n",
    "    offset += 1\n",
    "\n",
    "  if a.shape[0] >= out_dim:\n",
    "    a = a[:out_dim]\n",
    "    b = b[:out_dim]\n",
    "  else:\n",
    "    raise ValueError(\n",
    "        f'Could not generate enough unique connections: {a.shape[0]} <'\n",
    "        f' {out_dim}'\n",
    "    )\n",
    "\n",
    "  # Random permutation\n",
    "  perm = jax.random.permutation(key, out_dim)\n",
    "  a = a[perm]\n",
    "  b = b[perm]\n",
    "\n",
    "  return a, b\n",
    "\n",
    "\n",
    "def bin_op_all_combinations(a, b):\n",
    "  # Implementation of binary operations between two inputs for all the different operations\n",
    "  return jnp.stack(\n",
    "      [\n",
    "          jnp.zeros_like(a),\n",
    "          a * b,\n",
    "          a - a * b,\n",
    "          a,\n",
    "          b - a * b,\n",
    "          b,\n",
    "          a + b - 2 * a * b,\n",
    "          a + b - a * b,\n",
    "          1 - (a + b - a * b),\n",
    "          1 - (a + b - 2 * a * b),\n",
    "          1 - b,\n",
    "          1 - b + a * b,\n",
    "          1 - a,\n",
    "          1 - a + a * b,\n",
    "          1 - a * b,\n",
    "          jnp.ones_like(a),\n",
    "      ],\n",
    "      axis=-1,\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "def bin_op_s(a, b, i_s):\n",
    "  # Compute all possible operations\n",
    "  combinations = bin_op_all_combinations(\n",
    "      a, b\n",
    "  )  # Shape: (n_gate, n_possible_gates, 16)\n",
    "  \"\"\"\n",
    "    Calculate the weighted sum of all possible gate operations.\n",
    "    During training (soft decoding), the weights are the probabilities\n",
    "    of each gate type. During inference (hard decoding), the weights\n",
    "    are a one-hot vector representing the selected gate type.\n",
    "    \"\"\"\n",
    "  result = jax.numpy.sum(combinations * i_s[None, ...], axis=-1)\n",
    "  return result\n",
    "\n",
    "\n",
    "\n",
    "def decode_soft(weights):\n",
    "  # From the weights vector compute the probability distribution of choosing each gate using softmax\n",
    "  return nn.softmax(weights, axis=-1)\n",
    "\n",
    "\n",
    "def decode_hard(weights):\n",
    "  return jax.nn.one_hot(\n",
    "      jnp.argmax(weights, axis=-1), 16\n",
    "  )  # Return the gate with maximum probability.\n",
    "\n",
    "\n",
    "# Initialize gates as pass through gate\n",
    "def init_gates(\n",
    "    n,\n",
    "    num_gates=NUMBER_OF_GATES,\n",
    "    pass_through_gate=PASS_THROUGH_GATE,\n",
    "    default_pass_value=DEFAULT_PASS_VALUE,\n",
    "):\n",
    "  \"\"\"Initializes a gate matrix with default pass-through values.\n",
    "\n",
    "  Args:\n",
    "      n: The number of rows in the gate matrix.\n",
    "      num_gates: The number of gates (columns). Defaults to NUMBER_OF_GATES.\n",
    "      pass_through_gate: The index of the pass-through gate column. Defaults to\n",
    "        PASS_THROUGH_GATE.\n",
    "      default_pass_value: The default value for the pass-through gate. Defaults\n",
    "        to DEFAULT_PASS_VALUE.\n",
    "\n",
    "  Returns:\n",
    "      An array representing the initialized gate matrix.\n",
    "  \"\"\"\n",
    "  gates = jnp.zeros((n, num_gates))\n",
    "  gates = gates.at[:, pass_through_gate].set(default_pass_value)\n",
    "  return gates\n",
    "\n",
    "\n",
    "def init_gate_layer(key, in_dim, out_dim, connections):\n",
    "\n",
    "  # With 'random' connections the input of each gate are sampled randomly from the previous layer.\n",
    "  if connections == 'random':\n",
    "    key1, key2 = jax.random.split(key)\n",
    "\n",
    "    c = jax.random.permutation(key2, 2 * out_dim) % in_dim\n",
    "    c = jax.random.permutation(key1, in_dim)[c]\n",
    "\n",
    "    c = c.reshape(2, out_dim)\n",
    "\n",
    "    indices_a = c[0, :]\n",
    "    indices_b = c[1, :]\n",
    "\n",
    "  # With 'unique' connections each gate will have a different pair of inputs.\n",
    "  elif connections == 'unique':\n",
    "    indices_a, indices_b = get_unique_connections(in_dim, out_dim, key)\n",
    "\n",
    "  # With 'first_kernel' the connections are specifically designed to mimic the Moore neighborhood.\n",
    "  elif connections == 'first_kernel':\n",
    "    indices_a, indices_b = get_moore_connections(key)\n",
    "  else:\n",
    "    raise ValueError(f'Connection type {connections} not implemented')\n",
    "\n",
    "  wires = [indices_a, indices_b]\n",
    "  gate_logits = init_gates(out_dim)\n",
    "  return gate_logits, wires\n",
    "\n",
    "\n",
    "def init_logic_gate_network(hyperparams, params, wires, key):\n",
    "  for i, (in_dim, out_dim) in enumerate(\n",
    "      zip(hyperparams['layers'][:-1], hyperparams['layers'][1:])\n",
    "  ):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    gate_logits, gate_wires = init_gate_layer(\n",
    "        subkey, int(in_dim), int(out_dim), hyperparams['connections'][i]\n",
    "    )\n",
    "    params.append(gate_logits)\n",
    "    wires.append(gate_wires)\n",
    "\n",
    "\n",
    "def init_perceive_network(hyperparams, params, wires, key):\n",
    "  for i, (in_dim, out_dim) in enumerate(\n",
    "      zip(hyperparams['layers'][:-1], hyperparams['layers'][1:])\n",
    "  ):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    gate_logits, gate_wires = init_gate_layer(\n",
    "        subkey, int(in_dim), int(out_dim), hyperparams['connections'][i]\n",
    "    )\n",
    "    \"\"\"\n",
    "    Replicate the gate logits for each of the 'n_kernels' perception kernels.\n",
    "    This allows for parallel computation of the perception module,\n",
    "    as all kernels share the same underlying structure and wiring.\n",
    "    \"\"\"\n",
    "    params.append(\n",
    "        gate_logits.repeat(hyperparams['n_kernels'], axis=0).reshape(\n",
    "            hyperparams['n_kernels'], out_dim, NUMBER_OF_GATES\n",
    "        )\n",
    "    )\n",
    "    wires.append(gate_wires)\n",
    "\n",
    "\n",
    "# In the current implementation all the kernels share the same connection scheme\n",
    "def init_diff_logic_ca(hyperparams, key):\n",
    "\n",
    "  key, subkey = jax.random.split(key)\n",
    "  \"\"\"Initialize parameters for the update and perceive networks.\n",
    "\n",
    "   Each network's parameters will be stored as a list of gate logits.\n",
    "\n",
    "   'update':  List of gate logits for the update network.\n",
    "              Each element is a JAX array with shape (out_dim, 16),\n",
    "              where out_dim is the output dimension of the layer.\n",
    "   'perceive': List of gate logits for the perceive network.\n",
    "               Each element is a JAX array with shape (n_kernels * out_dim, 16),\n",
    "               where n_kernels is the number of perception kernels\n",
    "               and out_dim is the output dimension of the layer.\n",
    "  \"\"\"\n",
    "  params = {'update': [], 'perceive': []}\n",
    "\n",
    "  \"\"\"Initialize wiring for the update and perceive networks.\n",
    "\n",
    "   Each network's wiring will be stored as a list of connection indices.\n",
    "\n",
    "   'update':  List of connection indices for the update network.\n",
    "              Each element is a tuple of two JAX arrays (indices_a, indices_b)\n",
    "              representing the input indices for each gate.\n",
    "   'perceive': List of connection indices for the perceive network.\n",
    "               The perceive kernels share the same wiring, so this list\n",
    "               contains a single tuple of connection indices (indices_a, indices_b).\n",
    "  \"\"\"\n",
    "  wires = {'update': [], 'perceive': []}\n",
    "\n",
    "  # Initialize the gate\n",
    "  init_logic_gate_network(\n",
    "      hyperparams['update'], params['update'], wires['update'], subkey\n",
    "  )\n",
    "\n",
    "  key, subkey = jax.random.split(key)\n",
    "\n",
    "  # Initialize perceive vector\n",
    "  init_perceive_network(\n",
    "      hyperparams['perceive'], params['perceive'], wires['perceive'], subkey\n",
    "  )\n",
    "\n",
    "  return params, wires\n",
    "\n",
    "\n",
    "def run_layer(logits, wires, x, training):\n",
    "  \"\"\"Args:\n",
    "\n",
    "      x: input vector, shape (input_dim, 1).\n",
    "      wires: wire configuration, shape (out_dim, 1).\n",
    "      logits: gate parameters, shape (n_out, 16).\n",
    "\n",
    "  Returns:\n",
    "      If training is True, the expected gate values.\n",
    "      If training is False, the gates with maximum probability.\n",
    "  \"\"\"\n",
    "\n",
    "  a = x[..., wires[0]]\n",
    "  b = x[..., wires[1]]\n",
    "  logits = jax.lax.cond(training, decode_soft, decode_hard, logits)\n",
    "  out = bin_op_s(a, b, logits)\n",
    "  return out\n",
    "\n",
    "\n",
    "def run_update(params, wires, x, training):\n",
    "  for g, c in zip(params, wires):\n",
    "    x = run_layer(g, c, x, training)\n",
    "  return x\n",
    "\n",
    "\n",
    "def run_perceive(params, wires, x, training):\n",
    "  \"\"\"Applies a perception layer to a patch.\n",
    "\n",
    "  Args:\n",
    "      params: List of kernel parameters for each layer.\n",
    "      wires: List of wire configurations for each layer.\n",
    "      x: Input patch, shape [batch_size, patch_size, channel_size].\n",
    "      training: Boolean indicating training mode.\n",
    "\n",
    "  Returns:\n",
    "      Output feature vector, shape [batch_size, channel_size].\n",
    "  \"\"\"\n",
    "\n",
    "  # Apply each layer using vmap for kernel parallelism.\n",
    "  run_layer_map = jax.vmap(run_layer, in_axes=(0, None, 0, None))\n",
    "  x_prev = x\n",
    "  x = x.T  # [channel_size, batch_size, patch_size]\n",
    "\n",
    "  \"\"\"\n",
    "    Duplicate 'x' to create n_kernels copies for the first layer, which all share the same input.\n",
    "    Subsequent layers receive unique inputs.\n",
    "    \"\"\"\n",
    "  x = jnp.repeat(\n",
    "      x[None, ...], params[0].shape[0], axis=0\n",
    "  )  # [n_kernels, channel_size, batch_size, patch_size]\n",
    "\n",
    "  # Iterate through layers, applying kernels and wire configurations.\n",
    "  for g, c in zip(params, wires):\n",
    "    x = run_layer_map(g, c, x, training)\n",
    "\n",
    "  x = rearrange(\n",
    "      x, 'k c s -> (c s k)'\n",
    "  )  # [channel_size * patch_size * n_kernels]\n",
    "\n",
    "  return jnp.concatenate(\n",
    "      [x_prev[4, :], x], axis=-1\n",
    "  )  # Concatenate the original input.\n",
    "\n",
    "\n",
    "def run_circuit(params, wires, x, training):\n",
    "  x = run_perceive(params['perceive'], wires['perceive'], x, training)\n",
    "  x = run_update(params['update'], wires['update'], x, training)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QzCeEp-eu78Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0205 22:53:42.914490  180749 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0205 22:53:42.916361  180681 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 16)\n",
      "INPUT SHAPE (9, 16)\n",
      "OUTPUT SHAPE (1, 16)\n",
      "[CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "# Test cases\n",
    "key = jax.random.PRNGKey(0)\n",
    "key_init, key = jax.random.split(key, 2)\n",
    "n_kernels = 4\n",
    "layers = [128] * 10 + [64, 32, 16]\n",
    "connections = ['unique'] * len(layers)\n",
    "\n",
    "\"\"\"Recap: Structure of the hyperparameters:\n",
    "\n",
    "'update' is referring to the diff logic network of the update.\n",
    "- Layers:  number of gates per layer (with exception of the first layer\n",
    "           which represent the number of inputs)\n",
    "- Connections: specify the topology of the connections, how the gates\n",
    "           are connected. It can be mainly 'random', 'unique' or 'first_kernel'\n",
    "           which will be used to the first kernel layer.\n",
    "\n",
    "'perceive' is referring to the diff logic networks of the perception\n",
    "          (or \"convolutional kernels\").\n",
    "- Layers:  number of gates per layer (with exception of the first layer which\n",
    "           represent the number of inputs)\n",
    "- Connections: specify the topology of the connections, how the gates are connected.\n",
    "           It can be mainly 'random', 'unique' or 'first_kernel' which is used\n",
    "           only for the first kernel layer to enforce connections that mimic the cell-interactions.\n",
    "\n",
    "For simplicity, all perception kernels share the same structure (type of connections,\n",
    "number of layers, and number of gates per layer). They are evaluated in parallel.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "hyperparams = {\n",
    "    'update': {'layers': layers, 'connections': connections},\n",
    "    'perceive': {\n",
    "        'n_kernels': n_kernels,\n",
    "        'layers': [9, 8, 4, 2, 1],\n",
    "        'connections': ['first_kernel', 'unique', 'unique', 'unique'],\n",
    "    },\n",
    "}\n",
    "\n",
    "training = True\n",
    "params, wires = init_diff_logic_ca(hyperparams, key_init)\n",
    "print(\n",
    "    params['perceive'][0]\n",
    "    .reshape((\n",
    "        hyperparams['perceive']['n_kernels'],\n",
    "        hyperparams['perceive']['layers'][1],\n",
    "        16,\n",
    "    ))\n",
    "    .shape\n",
    ")\n",
    "grid_ch_dim = 16\n",
    "inp = jnp.ones((3 * 3, grid_ch_dim)).astype(jnp.float32)\n",
    "x = run_circuit(params, wires, inp, training)\n",
    "print('INPUT SHAPE', inp.shape)\n",
    "print('OUTPUT SHAPE', x.shape)\n",
    "\n",
    "# The output of the circuit is the new state for the central cell with shape (1, grid_ch_dim)\n",
    "assert x.shape == (1, grid_ch_dim)\n",
    "print(jax.devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0coNJrpHEkPx"
   },
   "outputs": [],
   "source": [
    "# @title get_grid_patches\n",
    "\n",
    "import os\n",
    "\n",
    "# If not already set in the notebook, allow env var override.\n",
    "if \"USE_CUDNN\" not in globals():\n",
    "  _use_cudnn_env = os.environ.get(\"DLCA_USE_CUDNN\", \"1\").lower()\n",
    "  USE_CUDNN = _use_cudnn_env not in (\"0\", \"false\", \"no\")\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1,2))\n",
    "def _get_grid_patches_cudnn(grid, patch_size, channel_dim, periodic):\n",
    "  # Given a grid NxN it creates a list of (patch_size x patch_size, channel_dim) patches.\n",
    "\n",
    "  pad_size = (patch_size - 1) // 2\n",
    "\n",
    "  # Two possible modes exist: with or without periodic boundary conditions.\n",
    "  padded_grid = jax.lax.cond(\n",
    "      periodic,\n",
    "      lambda g: jnp.pad(\n",
    "          g, ((pad_size, pad_size), (pad_size, pad_size), (0, 0)), mode=\"wrap\"\n",
    "      ),\n",
    "      lambda g: jnp.pad(\n",
    "          g,\n",
    "          ((pad_size, pad_size), (pad_size, pad_size), (0, 0)),\n",
    "          mode=\"constant\",\n",
    "          constant_values=0,\n",
    "      ),\n",
    "      grid,\n",
    "  )\n",
    "  padded_grid = jnp.expand_dims(padded_grid, axis=0)\n",
    "  patches = conv_general_dilated_patches(\n",
    "      padded_grid,\n",
    "      filter_shape=(patch_size, patch_size),\n",
    "      window_strides=(1, 1),\n",
    "      padding=\"VALID\",\n",
    "      dimension_numbers=(\"NHWC\", \"OIHW\", \"NHWC\"),\n",
    "  )[0]\n",
    "\n",
    "  # Rearrange to have (list, patch_size x patch_size, channel_dim)\n",
    "  patches = rearrange(patches, \"x y (c f) -> (x y) f c\", c=channel_dim)\n",
    "  return patches\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1,2))\n",
    "def _get_grid_patches_fallback(grid, patch_size, channel_dim, periodic):\n",
    "  # Given a grid NxN it creates a list of (patch_size x patch_size, channel_dim) patches.\n",
    "\n",
    "  pad_size = (patch_size - 1) // 2\n",
    "\n",
    "  # Two possible modes exist: with or without periodic boundary conditions.\n",
    "  padded_grid = jax.lax.cond(\n",
    "      periodic,\n",
    "      lambda g: jnp.pad(\n",
    "          g, ((pad_size, pad_size), (pad_size, pad_size), (0, 0)), mode=\"wrap\"\n",
    "      ),\n",
    "      lambda g: jnp.pad(\n",
    "          g,\n",
    "          ((pad_size, pad_size), (pad_size, pad_size), (0, 0)),\n",
    "          mode=\"constant\",\n",
    "          constant_values=0,\n",
    "      ),\n",
    "      grid,\n",
    "  )\n",
    "\n",
    "  # Extract patches without cuDNN to avoid GPU autotune failures on older GPUs.\n",
    "  n = grid.shape[0]\n",
    "  ys = jnp.arange(n, dtype=jnp.int32)\n",
    "  xs = jnp.arange(n, dtype=jnp.int32)\n",
    "\n",
    "  def slice_at(y, x):\n",
    "    return jax.lax.dynamic_slice(\n",
    "        padded_grid, (y, x, 0), (patch_size, patch_size, channel_dim)\n",
    "    )\n",
    "\n",
    "  patches = jax.vmap(lambda y: jax.vmap(lambda x: slice_at(y, x))(xs))(ys)\n",
    "\n",
    "  # Rearrange to have (list, patch_size x patch_size, channel_dim)\n",
    "  patches = patches.reshape(n * n, patch_size * patch_size, channel_dim)\n",
    "  return patches\n",
    "\n",
    "def get_grid_patches(grid, patch_size, channel_dim, periodic, use_cudnn=USE_CUDNN):\n",
    "  if use_cudnn:\n",
    "    return _get_grid_patches_cudnn(grid, patch_size, channel_dim, periodic)\n",
    "  return _get_grid_patches_fallback(grid, patch_size, channel_dim, periodic)\n",
    "\n",
    "def get_grid_patches_auto(grid, patch_size, channel_dim, periodic):\n",
    "  global USE_CUDNN\n",
    "  if USE_CUDNN:\n",
    "    try:\n",
    "      return _get_grid_patches_cudnn(grid, patch_size, channel_dim, periodic)\n",
    "    except Exception:\n",
    "      USE_CUDNN = False\n",
    "  return _get_grid_patches_fallback(grid, patch_size, channel_dim, periodic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c7JD6xxhu78Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid\n",
      " [[0. 0. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 0.]]\n",
      "[[[1. 1. 1.]\n",
      "  [1. 0. 1.]\n",
      "  [0. 1. 0.]]]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "N = 3\n",
    "C = 1\n",
    "patch_size = 3\n",
    "grid = random.randint(key, (N, N, C), minval=0, maxval=2).astype(jnp.float32)\n",
    "patches = get_grid_patches(\n",
    "    grid, patch_size=patch_size, channel_dim=grid.shape[-1], periodic=1\n",
    ")\n",
    "print(f\"Grid\\n {grid[..., 0]}\")\n",
    "print(patches[-1, :, 1].reshape(1, 3, 3))\n",
    "print(patches[-1:, 1].reshape(-1))\n",
    "\n",
    "assert patches.shape == (N * N, patch_size * patch_size, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rblos5LlWJWn"
   },
   "outputs": [],
   "source": [
    "# @title run iterations\n",
    "\n",
    "\n",
    "# patches = [batch_size, n_patches, patch_size x patch_size, channels]\n",
    "def v_run_circuit_patched(patches, params, wires, training):\n",
    "  run_circuit_patch = jax.vmap(\n",
    "      run_circuit, in_axes=(None, None, 0, None)\n",
    "  )  # vmap over the patches\n",
    "  x = run_circuit_patch(params, wires, patches, training)\n",
    "  return x\n",
    "\n",
    "@jax.jit\n",
    "def run_async(grid, params, wires, training, periodic, key):\n",
    "  patches = get_grid_patches(\n",
    "      grid, patch_size=3, channel_dim=grid.shape[-1], periodic=periodic\n",
    "  )\n",
    "  x_new = v_run_circuit_patched(patches, params, wires, training)\n",
    "  x_new = x_new.reshape(*grid.shape)\n",
    "  update_mask_f32 = (\n",
    "      jax.random.uniform(key, x_new[..., :1].shape) <= FIRE_RATE\n",
    "  ).astype(jax.numpy.float32)\n",
    "  x = grid * (1 - update_mask_f32) + x_new * update_mask_f32\n",
    "  return x\n",
    "\n",
    "@jax.jit\n",
    "def run_sync(grid, params, wires, training, periodic):\n",
    "  patches = get_grid_patches(\n",
    "      grid, patch_size=3, channel_dim=grid.shape[-1], periodic=periodic\n",
    "  )\n",
    "  x_new = v_run_circuit_patched(patches, params, wires, training)\n",
    "  x_new = x_new.reshape(*grid.shape)\n",
    "  return x_new\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=['num_steps', 'periodic', 'async_training'])\n",
    "def run_iter_nca(grid, params, wires, training, periodic, num_steps, async_training, key):\n",
    "  def body_fn(carry, i):\n",
    "    grid, key = carry\n",
    "    if async_training:\n",
    "      key, subkey = jax.random.split(key)\n",
    "      x = run_async(grid, params, wires, training, periodic, subkey)\n",
    "    else:\n",
    "      x = run_sync(grid, params, wires, training, periodic)\n",
    "    return (x, key), 0\n",
    "\n",
    "  (grid, key), _ = jax.lax.scan(\n",
    "      body_fn, (grid, key), jnp.arange(0, num_steps, 1)\n",
    "  )\n",
    "  return grid\n",
    "\n",
    "\n",
    "v_run_iter_nca = jax.vmap(\n",
    "    run_iter_nca, in_axes=(0, None, None, None, None, None, None, None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1JO6misXu78Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape  (1, 5, 5, 8)\n",
      "Output shape (1, 5, 5, 8)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test\n",
    "\n",
    "This test verifies that the output dimensions\n",
    "of the diffLogic CA correspond to the grid dimensions.\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 1\n",
    "N = 5\n",
    "C = 8\n",
    "n_kernels = 4\n",
    "layers = [128] * 10 + [64, 32, 16, C]\n",
    "connections = ['random'] * len(layers)\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "hyperparams = {\n",
    "    'update': {'layers': layers, 'connections': connections},\n",
    "    'perceive': {\n",
    "        'n_kernels': n_kernels,\n",
    "        'layers': [9, 8, 4, 2, 1],\n",
    "        'connections': ['first_kernel', 'unique', 'unique', 'unique'],\n",
    "    },\n",
    "}\n",
    "params, wires = init_diff_logic_ca(hyperparams, key)\n",
    "grid = random.randint(key, (batch_size, N, N, C), minval=0, maxval=2).astype(\n",
    "    jnp.float32\n",
    ")\n",
    "\n",
    "print('Input shape ', grid.shape)\n",
    "\n",
    "# Run for 10 iterations\n",
    "num_steps = 10\n",
    "key, subkey = jax.random.split(key)\n",
    "x = v_run_iter_nca(grid, params, wires, True, True, num_steps, False, subkey)\n",
    "\n",
    "print('Output shape', x.shape)\n",
    "\n",
    "assert x.shape == (batch_size, N, N, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainState = namedtuple('TrainState', 'param opt_state key')\n",
    "\n",
    "def init_state(hyperparams, opt, seed):\n",
    "  key = random.PRNGKey(seed)\n",
    "  key, subkey = random.split(key, 2)\n",
    "  params, wires = init_diff_logic_ca(hyperparams, subkey)\n",
    "  opt_state = opt.init(params)\n",
    "  return TrainState(params, opt_state, key), wires\n",
    "\n",
    "\n",
    "def loss_f(params, wires, train_x, train_y, periodic, num_steps, async_training, key):\n",
    "  def eval(params, training):\n",
    "    y = v_run_iter_nca(\n",
    "        train_x, params, wires, training, periodic, num_steps, async_training, key\n",
    "    )\n",
    "    return jax.numpy.square(y[..., 0] - train_y[..., 0]).sum()\n",
    "\n",
    "  return eval(params, 1), {'hard': eval(params, 0)}\n",
    "\n",
    "val_and_grad = jax.value_and_grad(loss_f, argnums=0, has_aux=True)\n",
    "\n",
    "# Helper function that can be used for clipping parameters.\n",
    "upd_f = lambda p: p\n",
    "\n",
    "@partial(jax.jit, static_argnums=(4, 5, 6))\n",
    "def train_step(\n",
    "    train_state, train_x, train_y, wires, periodic, num_steps, async_training\n",
    "): \n",
    "  params, opt_state, key = train_state\n",
    "  key, k1 = jax.random.split(key, 2)\n",
    "  (loss, hard), dx = val_and_grad(\n",
    "      params, wires, train_x, train_y, periodic, num_steps, async_training, k1\n",
    "  )\n",
    "  dx, opt_state = opt.update(dx, opt_state, params)\n",
    "  new_params = optax.apply_updates(params, dx)\n",
    "  new_params = upd_f(new_params)\n",
    "  return TrainState(new_params, opt_state, key), loss, hard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_finQuMKUSa"
   },
   "outputs": [],
   "source": [
    "# @title Learning Chekerboard with Asynchronously Training\n",
    "hyperparams = {'perceive': {}, 'update': {}}\n",
    "hyperparams['seed'] = 23\n",
    "hyperparams['lr'] = 0.05\n",
    "hyperparams['batch_size'] = 1\n",
    "hyperparams['num_epochs'] = 800\n",
    "hyperparams['num_steps'] = 50\n",
    "hyperparams['channels'] = 8\n",
    "hyperparams['periodic'] = 0\n",
    "hyperparams['perceive']['n_kernels'] = 16\n",
    "hyperparams['perceive']['layers'] = [9, 8, 4, 2]\n",
    "hyperparams['perceive']['connections'] = [\n",
    "    'first_kernel',\n",
    "    'unique',\n",
    "    'unique',\n",
    "    'unique',\n",
    "]\n",
    "init = (\n",
    "    hyperparams['perceive']['n_kernels']\n",
    "    * hyperparams['channels']\n",
    "    * hyperparams['perceive']['layers'][-1]\n",
    "    + hyperparams['channels']\n",
    ")\n",
    "hyperparams['update']['layers'] = (\n",
    "    [init] + [256] * 14 + [128, 64, 32, 16, 8, hyperparams['channels']]\n",
    ")\n",
    "hyperparams['update']['connections'] = ['unique'] * len(\n",
    "    hyperparams['update']['layers']\n",
    ")\n",
    "hyperparams['async_training'] = True\n",
    "\n",
    "\n",
    "\n",
    "# Create optimizer\n",
    "opt = optax.chain(\n",
    "    optax.clip(100.0),  # Clips by value\n",
    "    optax.adamw(\n",
    "        learning_rate=hyperparams['lr'], b1=0.9, b2=0.99, weight_decay=1e-2\n",
    "    ),\n",
    ")\n",
    "\n",
    "train_state, wires = init_state(hyperparams, opt, hyperparams['seed'])\n",
    "key = random.PRNGKey(hyperparams['seed'])\n",
    "\n",
    "loss_soft = []\n",
    "loss_hard = []\n",
    "\n",
    "img = create_checkerboard((TARGET_SIZE_ASYNC, TARGET_SIZE_ASYNC), shape_size=2)\n",
    "\n",
    "train_y = jnp.zeros(\n",
    "    shape=(\n",
    "        hyperparams['batch_size'],\n",
    "        TARGET_SIZE_ASYNC,\n",
    "        TARGET_SIZE_ASYNC,\n",
    "        hyperparams['channels'],\n",
    "    )\n",
    ")\n",
    "train_y = train_y.at[:, :, :, 0].set(img)\n",
    "\n",
    "for i in range(hyperparams['num_epochs']):\n",
    "  key, sample_key = random.split(key, 2)\n",
    "  train_x = random.randint(\n",
    "      key=sample_key,\n",
    "      shape=(\n",
    "          hyperparams['batch_size'],\n",
    "          TARGET_SIZE_ASYNC,\n",
    "          TARGET_SIZE_ASYNC,\n",
    "          hyperparams['channels'],\n",
    "      ),\n",
    "      minval=0,\n",
    "      maxval=2,  # maxval is exclusive\n",
    "  ).astype(jnp.float32)\n",
    "\n",
    "  train_state, soft_loss, hard_loss = train_step(\n",
    "      train_state,\n",
    "      train_x,\n",
    "      train_y,\n",
    "      wires,\n",
    "      hyperparams['periodic'],\n",
    "      hyperparams['num_steps'],\n",
    "      hyperparams['async_training'],\n",
    "  )\n",
    "  loss_soft.append(soft_loss)\n",
    "  loss_hard.append(hard_loss['hard'])\n",
    "\n",
    "  if i % 100 == 0:\n",
    "    clear_output(wait=True)\n",
    "    plot_training_progress(loss_soft, loss_hard, 1)\n",
    "    print(i, soft_loss, hard_loss['hard'])\n",
    "\n",
    "clear_output(wait=True)\n",
    "plot_training_progress(loss_soft, loss_hard, 1, 'checkerboard_async_loss.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIeEm3ZdKgve"
   },
   "outputs": [],
   "source": [
    "\"\"\"Checkerboard pattern with Asynchronous training\n",
    "\n",
    "Visualization of the recurrent dynamics of the learned circuit.\n",
    "\"\"\"\n",
    "\n",
    "grid = random.randint(\n",
    "    key=sample_key,\n",
    "    shape=(TARGET_SIZE_ASYNC, TARGET_SIZE_ASYNC, hyperparams['channels']),\n",
    "    minval=0,\n",
    "    maxval=2,\n",
    ").astype(jnp.float32)\n",
    "\n",
    "params, opt_state, key = train_state\n",
    "wires = wires\n",
    "training = False\n",
    "periodic = False\n",
    "\n",
    "frames = []\n",
    "\n",
    "nca_state = grid\n",
    "frames.append(zoom(nca_state[:, :, 0], 16))\n",
    "for i in range(50):\n",
    "  key, subkey = jax.random.split(key)\n",
    "  nca_state = run_async(nca_state, params, wires, training, periodic, subkey)\n",
    "  frames.append(zoom(nca_state[:, :, 0], 16))\n",
    "\n",
    "visualize(frames, 'board_async.gif', 5)\n",
    "Image('board_async.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czeKuh39KpLb"
   },
   "outputs": [],
   "source": [
    "\"\"\"Checkerboard pattern\n",
    "\n",
    "Visualization of the recurrent dynamics of the learned circuit\n",
    "using three different seed to compute asynchrnous masks.\n",
    "\"\"\"\n",
    "\n",
    "params, opt_state, key = train_state\n",
    "wires = wires\n",
    "training = False\n",
    "periodic = False\n",
    "\n",
    "frames = []\n",
    "nca_state_1 = grid.copy()\n",
    "nca_state_2 = grid.copy()\n",
    "nca_state_3 = grid.copy()\n",
    "\n",
    "ZOOM = 32\n",
    "separator = jnp.zeros((TARGET_SIZE_ASYNC * ZOOM, 5)) + 0.5\n",
    "frames.append(\n",
    "    np.hstack([\n",
    "        zoom(nca_state_1[:, :, 0], ZOOM),\n",
    "        separator,\n",
    "        zoom(nca_state_2[:, :, 0], ZOOM),\n",
    "        separator,\n",
    "        zoom(nca_state_3[:, :, 0], ZOOM),\n",
    "    ])\n",
    ")\n",
    "\n",
    "for i in range(50):\n",
    "  key, subkey = jax.random.split(key)\n",
    "  nca_state_1 = run_async(\n",
    "      nca_state_1, params, wires, training, periodic, subkey\n",
    "  )\n",
    "  key, subkey = jax.random.split(key)\n",
    "  nca_state_2 = run_async(\n",
    "      nca_state_2, params, wires, training, periodic, subkey\n",
    "  )\n",
    "  key, subkey = jax.random.split(key)\n",
    "  nca_state_3 = run_async(\n",
    "      nca_state_3, params, wires, training, periodic, subkey\n",
    "  )\n",
    "  frames.append(\n",
    "      np.hstack([\n",
    "          zoom(nca_state_1[:, :, 0], ZOOM),\n",
    "          separator,\n",
    "          zoom(nca_state_2[:, :, 0], ZOOM),\n",
    "          separator,\n",
    "          zoom(nca_state_3[:, :, 0], ZOOM),\n",
    "      ])\n",
    "  )\n",
    "\n",
    "# Display the concatenated video\n",
    "visualize(frames, 'board_async_multi.gif', 5)\n",
    "Image('board_async_multi.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQcSajA1LY-U"
   },
   "outputs": [],
   "source": [
    "\"\"\"Checkerboard pattern with Asynchrnous Training: testing self-healing\n",
    "\n",
    "We simulate the learned circuit on a grid that is four times\n",
    "bigger and for four times as many timesteps as during training.\n",
    "\n",
    "Similar to the previous test by deactivating a 10x10 cell center\n",
    "to evaluate pattern reconstruction under cell damage.\n",
    "\n",
    "We now re-activate the cells after 100 steps to evaluate self-healing.\n",
    "\"\"\"\n",
    "\n",
    "grid = random.randint(\n",
    "    key=sample_key,\n",
    "    shape=(TARGET_SIZE_ASYNC * 4, TARGET_SIZE_ASYNC * 4, hyperparams['channels']),\n",
    "    minval=0,\n",
    "    maxval=2,\n",
    ").astype(jnp.float32)\n",
    "\n",
    "params, opt_state, key = train_state\n",
    "wires = wires\n",
    "training = False\n",
    "periodic = False\n",
    "\n",
    "print(grid.shape)\n",
    "frames = []\n",
    "nca_state = grid\n",
    "frames.append(zoom(nca_state[:, :, 0], 8))\n",
    "for i in range(50 * 4):\n",
    "  key, subkey = jax.random.split(key)\n",
    "  nca_state = run_async(nca_state, params, wires, training, periodic, subkey)\n",
    "  if i < 50 * 2:\n",
    "    nca_state = nca_state.at[\n",
    "        nca_state.shape[0] // 2 - 10 : nca_state.shape[0] // 2 + 10,\n",
    "        nca_state.shape[1] // 2 - 10 : nca_state.shape[1] // 2 + 10,\n",
    "        :,\n",
    "    ].set(0)\n",
    "\n",
    "  frames.append(zoom(nca_state[:, :, 0], 8))\n",
    "\n",
    "visualize(frames, 'healing_board.gif', 5)\n",
    "Image('healing_board.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jktJePakij6v"
   },
   "outputs": [],
   "source": [
    "\"\"\"Checkerboard pattern: testing robustness\n",
    "\n",
    "We simulate the learned circuit on a 64x64 grid for 4000 steps.\n",
    "At each time step, we randomly reset a 10x10 square and measure\n",
    "the loss function to test how quickly the asynchronously trained\n",
    "circuit recovers.\n",
    "\n",
    "We will use the computed err_async to compare it to the synchronously\n",
    "trained circuit.\n",
    "\"\"\"\n",
    "\n",
    "grid = random.randint(\n",
    "    key=sample_key, shape=(64, 64, hyperparams['channels']), minval=0, maxval=2\n",
    ").astype(jnp.float32)\n",
    "\n",
    "params, opt_state, key = train_state\n",
    "wires = wires\n",
    "training = False\n",
    "periodic = False\n",
    "target = create_checkerboard((64, 64), shape_size=2)\n",
    "frames = []\n",
    "nca_state = grid\n",
    "frames.append(zoom(nca_state[:, :, 0], 8))\n",
    "err_async = []\n",
    "for i in range(4000):\n",
    "  key, k1, k2, k3 = random.split(key, 4)\n",
    "  nca_state = run_async(\n",
    "      nca_state, params, wires, training, periodic, k1\n",
    "  )  # async at inference\n",
    "\n",
    "  index_x = random.randint(k2, shape=(), minval=0, maxval=nca_state.shape[0])\n",
    "  index_y = random.randint(k3, shape=(), minval=0, maxval=nca_state.shape[0])\n",
    "\n",
    "  i_x_0 = index_x\n",
    "  i_x_1 = (\n",
    "      index_x + 10 if index_x + 10 <= nca_state.shape[0] else nca_state.shape[0]\n",
    "  )\n",
    "  i_y_0 = index_y\n",
    "  i_y_1 = (\n",
    "      index_y + 10 if index_y + 10 <= nca_state.shape[1] else nca_state.shape[1]\n",
    "  )\n",
    "  nca_state = nca_state.at[i_x_0:i_x_1, i_y_0:i_y_1, :].set(0)\n",
    "  frames.append(zoom(nca_state[:, :, 0], 8))\n",
    "  err = jax.numpy.abs(nca_state[:, :, 0] - target)\n",
    "  err_async.append(err.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1k7-UHqGzau2"
   },
   "outputs": [],
   "source": [
    "\"\"\"Testing Robustness\n",
    "\n",
    "This plot shows the robustness error difference between\n",
    "synchronous and asynchronous training, as previously calculated.\n",
    "\"\"\"\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "ax.plot(\n",
    "    err_async[:4000], color='#2E86C1', linewidth=2, label='Async', alpha=0.9\n",
    ")\n",
    "\n",
    "ax.plot(err_sync[:4000], color='#E74C3C', linewidth=2, label='Sync', alpha=0.9)\n",
    "\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Iterations', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Error', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_title(\n",
    "    'Error Comparison - Async vs Sync', fontsize=14, fontweight='bold', pad=20\n",
    ")\n",
    "\n",
    "ax.legend(\n",
    "    frameon=True, fancybox=True, shadow=True, fontsize=10, loc='upper right'\n",
    ")\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "  spine.set_linewidth(1.5)\n",
    "\n",
    "ax.set_facecolor('#f8f9fa')\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = 'async_sync.svg'\n",
    "\n",
    "with open(save_path, 'wb') as f:\n",
    "  plt.savefig(f, format='svg')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
